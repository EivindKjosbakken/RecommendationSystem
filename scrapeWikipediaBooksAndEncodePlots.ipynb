{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eivin\\Documents\\Programming\\TryFolder\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tensorflow_hub as hub\n",
    "from scipy.spatial import distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def getSoup(url):\n",
    "    response = requests.get(url=url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def getTitleAndSummaryFromWikipediaPage(soup):\n",
    "    title = soup.select(\"#firstHeading\")[0].text\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3'])\n",
    "\n",
    "    relevantHeader = None\n",
    "    for i in range(len(headers)):\n",
    "        firstString = re.split('[^a-zA-Z]', headers[i].getText())[0]\n",
    "        if (firstString.lower() == \"plot\" or firstString.lower() == \"summary\"):\n",
    "            relevantHeader = (headers[i])\n",
    "    if (relevantHeader == None):\n",
    "        return None\n",
    "    summary = \"\"\n",
    "    for elem in relevantHeader.next_siblings:\n",
    "        if elem.name and elem.name.startswith('h'):\n",
    "            # stop at next header\n",
    "            break\n",
    "        if elem.name == 'p':\n",
    "            summary += (elem.get_text())+\" \"\n",
    "            #f.write(elem.get_text() + u'\\n')\n",
    "    return title, summary\n",
    "\n",
    "def removeStopWords(string) -> str:\n",
    "    \"\"\"takes in a string, removes stop words from it, and returns the string without stopwords\"\"\"\n",
    "    word_tokens = word_tokenize(string)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "def getAllLinksFromPage(soup):\n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "    res = []\n",
    "    for ele in allLinks:\n",
    "        if (ele.has_attr(\"href\") and ele['href'].find(\"/wiki/\") != -1):\n",
    "            res.append(ele)\n",
    "    return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get wikipedia url from a wikidata id\n",
    "def get_wikipedia_url_from_wikidata_id(wikidata_id, lang='en', debug=False):\n",
    "    \"\"\"code from: https://stackoverflow.com/questions/37079989/how-to-get-wikipedia-page-from-wikidata-id\"\"\"\n",
    "    import requests\n",
    "    from requests import utils\n",
    "\n",
    "    url = (\n",
    "        'https://www.wikidata.org/w/api.php'\n",
    "        '?action=wbgetentities'\n",
    "        '&props=sitelinks/urls'\n",
    "        f'&ids={wikidata_id}'\n",
    "        '&format=json')\n",
    "    json_response = requests.get(url).json()\n",
    "    if debug: print(wikidata_id, url, json_response) \n",
    "\n",
    "    entities = json_response.get('entities')    \n",
    "    if entities:\n",
    "        entity = entities.get(wikidata_id)\n",
    "        if entity:\n",
    "            sitelinks = entity.get('sitelinks')\n",
    "            if sitelinks:\n",
    "                if lang:\n",
    "                    # filter only the specified language\n",
    "                    sitelink = sitelinks.get(f'{lang}wiki')\n",
    "                    if sitelink:\n",
    "                        wiki_url = sitelink.get('url')\n",
    "                        if wiki_url:\n",
    "                            return requests.utils.unquote(wiki_url)\n",
    "                else:\n",
    "                    # return all of the urls\n",
    "                    wiki_urls = {}\n",
    "                    for key, sitelink in sitelinks.items():\n",
    "                        wiki_url = sitelink.get('url')\n",
    "                        if wiki_url:\n",
    "                            wiki_urls[key] = requests.utils.unquote(wiki_url)\n",
    "                    return wiki_urls\n",
    "    return None   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "allIds = pd.read_csv(\"wikidataLitteratureWorkIds.csv\")\n",
    "allIdsNp = allIds.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data:\n",
    "\n",
    "def preProcessData(data : list, numberOfElements):\n",
    "    \"\"\"takes in the data from sparQL, and converts it to just a list of ids. Returs array of length numberOfElements\"\"\"\n",
    "    ids = []\n",
    "    for idx, ele in enumerate(data):\n",
    "        if (idx >= numberOfElements):\n",
    "            return ids\n",
    "        url = ele[0]\n",
    "        urlArr = url.split(\"/\")\n",
    "        wikidataId = urlArr[-1]\n",
    "        ids.append(wikidataId)\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = preProcessData(allIdsNp, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(listOfIdsToScrape, numberOfElementsToScrape, filename):\n",
    "    titleAndEncodedSummaries = dict()\n",
    "    numberOfSummariesAdded = 0\n",
    "\n",
    "    for idx, idToScrape in enumerate(listOfIdsToScrape):\n",
    "        if (numberOfSummariesAdded >= numberOfElementsToScrape):\n",
    "            break\n",
    "        try:\n",
    "            wikipediaUrl = get_wikipedia_url_from_wikidata_id(idToScrape) #\n",
    "            soup = getSoup(wikipediaUrl)\n",
    "            res = getTitleAndSummaryFromWikipediaPage(soup)\n",
    "            if (res is None):\n",
    "                continue\n",
    "            title, summary = res\n",
    "            numberOfSummariesAdded += 1\n",
    "            nonStopWordSummary = removeStopWords(summary)\n",
    "            embedding = embed([nonStopWordSummary]).numpy().tolist() #make embedding, convert it to np array\n",
    "            titleAndEncodedSummaries[title] = embedding\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #end by writing the data to file\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(titleAndEncodedSummaries, f)\n",
    "    f.close()\n",
    "    print(f\"Added {numberOfSummariesAdded} books\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 books\n"
     ]
    }
   ],
   "source": [
    "#scrape\n",
    "scrape(ids, 3, \"bookTitleAndSummaries.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('bookTitleAndSummaries.json')\n",
    "obj = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Lancelot, the Knight of the Cart', 'The Walrus and the Carpenter', '150 000 000'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
